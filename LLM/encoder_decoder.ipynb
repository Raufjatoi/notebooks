{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded:  [65, 98, 100, 117, 108, 32, 82, 97, 117, 102]\n",
      "decoded:  Abdul Rauf\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    return [ord(char) for char in text]\n",
    "\n",
    "def decode(nums):\n",
    "    return ''.join(chr(num) for num in nums)\n",
    "\n",
    "text = \"Abdul Rauf\"\n",
    "encoded = encode(text)\n",
    "decoded = decode(encoded)\n",
    "\n",
    "print('encoded: ', encoded)\n",
    "print('decoded: ', decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded : 72 105 32 44 32 73 109 32 114 97 117 102 33\n",
      "decoded : Hi , Im rauf!\n"
     ]
    }
   ],
   "source": [
    "def encode(text):\n",
    "    return ' '.join(str(ord(char)) for char in text)\n",
    "\n",
    "def decode(nums):\n",
    "    return ''.join(chr(int(num)) for num in nums.split())\n",
    "\n",
    "\n",
    "text = \"Hi , Im rauf!\"\n",
    "encoded = encode(text)\n",
    "decoded = decode(encoded)\n",
    "\n",
    "print(\"encoded :\", encoded)\n",
    "print(\"decoded :\", decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rauf is an AI and ML engineer who studies in QUEST University.\n"
     ]
    }
   ],
   "source": [
    "def decode(nums):\n",
    "    return ''.join(chr(int(num)) for num in nums.split())\n",
    "\n",
    "nums = \"82 97 117 102 32 105 115 32 97 110 32 65 73 32 97 110 100 32 77 76 32 101 110 103 105 110 101 101 114 32 119 104 111 32 115 116 117 100 105 101 115 32 105 110 32 81 85 69 83 84 32 85 110 105 118 101 114 115 105 116 121 46\"\n",
    "text = decode(nums)\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# more like a transformer ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: i like music\n",
      "Token IDs: [3, 4, 7]\n",
      "Embeddings: [[ 0.46548567  0.0170458   0.14591807 -1.1541701  -1.33036509 -2.13509428\n",
      "  -1.82943882  0.17930458]\n",
      " [-1.58710845  0.56924933 -0.63067732 -1.65797882 -1.28127458  2.35046445\n",
      "   0.59150691  0.97028405]\n",
      " [ 0.00248547 -0.40309693  1.29835778  0.39032915  0.48558573  0.19364147\n",
      "   0.74993737  0.23244141]]\n",
      "Decoded text: i like music\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# tokenizer\n",
    "class tokenizer:\n",
    "    def __init__(self):\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = {}\n",
    "    \n",
    "    def fit(self, text):\n",
    "        tokens = set(text.split())\n",
    "        for idx, token in enumerate(tokens):\n",
    "            self.token_to_id[token] = idx\n",
    "            self.id_to_token[idx] = token\n",
    "    \n",
    "    def tokenize(self, text):\n",
    "        return [self.token_to_id[token] for token in text.split()]\n",
    "    \n",
    "    def detokenize(self, token_ids):\n",
    "        return ' '.join(self.id_to_token[id] for id in token_ids)\n",
    "\n",
    "# encoder\n",
    "class encoder:\n",
    "    def __init__(self, vocab_size, embed_dim=8):\n",
    "        # an embedding matrix randomly initialized\n",
    "        self.embeddings = np.random.randn(vocab_size, embed_dim)\n",
    "    \n",
    "    def encode(self, token_ids):\n",
    "        return np.array([self.embeddings[id] for id in token_ids])\n",
    "\n",
    "# decoder\n",
    "class decoder:\n",
    "    def __init__(self, tokenizer, encoder):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoder = encoder\n",
    "    \n",
    "    def decode(self, embeddings):\n",
    "        # closest token for each embedding\n",
    "        token_ids = [np.argmin(np.linalg.norm(self.encoder.embeddings - embed, axis=1)) for embed in embeddings]\n",
    "        return self.tokenizer.detokenize(token_ids)\n",
    "\n",
    "text = \"Im rauf and i like to listen music\"\n",
    "tokenizer = tokenizer()\n",
    "tokenizer.fit(text)\n",
    "\n",
    "token_ids = tokenizer.tokenize(\"i like music\")\n",
    "\n",
    "encoder = encoder(vocab_size=len(tokenizer.token_to_id))\n",
    "embeddings = encoder.encode(token_ids)\n",
    "\n",
    "decoder = decoder(tokenizer, encoder)\n",
    "decoded_text = decoder.decode(embeddings)\n",
    "\n",
    "print(\"Original text:\", \"i like music\")\n",
    "print(\"Token IDs:\", token_ids)\n",
    "print(\"Embeddings:\", embeddings)\n",
    "print(\"Decoded text:\", decoded_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
